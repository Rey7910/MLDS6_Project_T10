{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), '..')))\n",
    "\n",
    "from src.MLDS6_Assets.preprocessing import countFilesinFolder\n",
    "from src.MLDS6_Assets.visualization import plotDataPartition\n",
    "from src.MLDS6_Assets.models import trainTestSplitImages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Partición de datos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para la partición de datos, vamos a tomar de base los datos unidos y balanceados en un único dataset. Para realizar la partición, usaremos la función train test split de sklearn y mantener el esquema de carpetas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selección y diseño de modelos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para abordar el problema de clasificación de imágenes en el diagnóstico de neumonía a partir de radiografías, hemos seleccionado los modelos VGG16 y EfficientNetB0, ambos disponibles en TensorFlow. Esta elección se justifica debido a que ambos modelos son redes neuronales convolucionales bien probadas en tareas de reconocimiento de imágenes y permiten capturar características visuales esenciales en radiografías. VGG16, con su arquitectura profunda y secuencial, es eficaz en detectar patrones sutiles y texturas asociadas a los signos de neumonía, como opacidades pulmonares; mientras que EfficientNetB0 se distingue por su capacidad de procesamiento optimizada y eficiente, lo que es ideal para implementaciones en tiempo real sin comprometer precisión. Además, se implementará transfer learning utilizando modelos preentrenados, lo que permitirá aprovechar características previamente aprendidas en grandes conjuntos de datos, acelerando el entrenamiento y mejorando la generalización en nuestro conjunto de datos específico. Al combinar ambos modelos, podemos lograr un balance entre precisión y velocidad, mejorando la fiabilidad diagnóstica. Además, ambos modelos se pueden adaptar añadiendo capas densas y técnicas como dropout o normalización para ajustarse mejor a las necesidades específicas del conjunto de datos, maximizando así el rendimiento en la tarea de clasificación.\n",
    "\n",
    "### Carga de modelos pre entrenados y configuracion de mlops\n",
    "\n",
    "A continuación, comenzamos definiendo las funciones necesarias para pre-cargar los modelos VGG16 y EfficientNetB0, los cuales serán modificados y adaptados para abordar el problema en cuestión. Es importante señalar que, aunque se crearán las funciones correspondientes, no se ejecutarán hasta el final. Esto se hará con el propósito de mantener un orden claro y ejecutar todo de manera estructurada en la última fase del proceso."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
    "import matplotlib.pyplot as plt\n",
    "import mlflow\n",
    "import mlflow.tensorflow\n",
    "\n",
    "# Configuración inicial de MLflow\n",
    "mlflow.set_tracking_uri(\"mlruns/\")\n",
    "mlflow.tensorflow.autolog()  # Activar autologging para TensorFlow\n",
    "\n",
    "# Función para cargar un modelo base preentrenado\n",
    "def load_pretrained_model(model_name, input_shape):\n",
    "    if model_name == 'VGG16':\n",
    "        base_model = tf.keras.applications.VGG16(weights='imagenet', include_top=False, input_shape=input_shape)\n",
    "    elif model_name == 'EfficientNetB0':\n",
    "        base_model = tf.keras.applications.EfficientNetB0(weights='imagenet', include_top=False, input_shape=input_shape)\n",
    "    else:\n",
    "        raise ValueError(\"Model name not recognized. Choose 'VGG16' or 'EfficientNetB0'.\")\n",
    "\n",
    "    for layer in base_model.layers:\n",
    "        layer.trainable = False\n",
    "\n",
    "    return base_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Personalizacion de los modelos\n",
    "Recordemos que nuestro objetivo es adaptar estos modelos a una arquitectura diferente para tener una mejor aproximacion al problema, por lo cual, generamos una funcion que genere dos configuraciones de capas densas al final de este que nos permita generar distintos modelos para comparar. Con esto, la idea es generar cuatro modelos entrenables diferentes que se puedan entrenar y evaluar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definición de modelos personalizados\n",
    "def config_model(base_model, version, lr=1e-3):\n",
    "    input_layer = layers.Input(shape=(765, 500, 3))\n",
    "    x = base_model(input_layer)\n",
    "    x = layers.GlobalAveragePooling2D()(x)\n",
    "\n",
    "    if version == 1:\n",
    "        x = layers.Dense(512, activation='relu')(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "        x = layers.Dropout(0.5)(x)\n",
    "        x = layers.Dense(256, activation='relu')(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "        x = layers.Dropout(0.3)(x)\n",
    "    elif version == 2:\n",
    "        x = layers.Dense(128, activation='relu')(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "        x = layers.Dropout(0.5)(x)\n",
    "        x = layers.Dense(64, activation='relu')(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "        x = layers.Dropout(0.3)(x)\n",
    "        x = layers.Dense(32, activation='relu')(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "        x = layers.Dropout(0.2)(x)\n",
    "    else:\n",
    "        raise ValueError(\"Version not recognized. Choose 1 or 2.\")\n",
    "\n",
    "    output_layer = layers.Dense(3, activation='softmax')(x)\n",
    "    model = models.Model(inputs=input_layer, outputs=output_layer)\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=lr),\n",
    "        loss='categorical_crossentropy',\n",
    "        metrics=['accuracy',\n",
    "                 tf.keras.metrics.TopKCategoricalAccuracy(k=2, name='top_2_accuracy'),\n",
    "                 tf.keras.metrics.AUC(multi_label=True)]\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creacion de callbacks\n",
    "se configuran funciones automáticas que se ejecutan durante el entrenamiento para optimizar el proceso. Estas incluyen guardar el mejor modelo (ModelCheckpoint), detener el entrenamiento si no hay mejora en un tiempo (EarlyStopping), y reducir el learning rate si el modelo deja de mejorar (ReduceLROnPlateau). Estas herramientas ayudan a evitar el sobreajuste y a ajustar el modelo de forma más eficiente.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función para configurar callbacks\n",
    "def create_callbacks(model_name):\n",
    "    os.makedirs(\"checkpoints\", exist_ok=True)\n",
    "    checkpoint_path = f\"checkpoints/best_{model_name}.h5\"\n",
    "\n",
    "    checkpoint = ModelCheckpoint(\n",
    "        checkpoint_path, monitor='val_accuracy', mode='max', save_best_only=True, verbose=1\n",
    "    )\n",
    "\n",
    "    early_stopping = EarlyStopping(\n",
    "        monitor='val_loss', patience=10, restore_best_weights=True, verbose=1\n",
    "    )\n",
    "\n",
    "    reduce_lr = ReduceLROnPlateau(\n",
    "        monitor='val_loss', factor=0.2, patience=5, min_lr=1e-6, verbose=1\n",
    "    )\n",
    "\n",
    "    return [checkpoint, early_stopping, reduce_lr]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entrenamiento del modelo usando Mlflow\n",
    "Una vez definido todo lo anterior, definimos una funcion para que mlflow para que empiece a realizar los entrenamientos correspondientes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función para entrenar el modelo\n",
    "def train_model(model, model_name, train_data, validation_data, epochs=10):\n",
    "    callbacks = create_callbacks(model_name)\n",
    "\n",
    "    with mlflow.start_run(run_name=model_name):\n",
    "        history = model.fit(\n",
    "            train_data,\n",
    "            epochs=epochs,\n",
    "            validation_data=validation_data,\n",
    "            callbacks=callbacks,\n",
    "            verbose=1\n",
    "        )\n",
    "\n",
    "    return history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Historial de entrenamiento\n",
    "Es importante tambien destacar los historiales de entrenamiento, por lo cual se realiza el graficado tal como se ve a continuacion:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función para visualizar el historial de entrenamiento\n",
    "def plot_training_history(history, model_name):\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "    ax1.plot(history.history['accuracy'], label='train')\n",
    "    ax1.plot(history.history['val_accuracy'], label='validation')\n",
    "    ax1.set_title(f'{model_name} - Accuracy')\n",
    "    ax1.set_xlabel('Epoch')\n",
    "    ax1.set_ylabel('Accuracy')\n",
    "    ax1.legend()\n",
    "\n",
    "    ax2.plot(history.history['loss'], label='train')\n",
    "    ax2.plot(history.history['val_loss'], label='validation')\n",
    "    ax2.set_title(f'{model_name} - Loss')\n",
    "    ax2.set_xlabel('Epoch')\n",
    "    ax2.set_ylabel('Loss')\n",
    "    ax2.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{model_name}_training_history.png\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejecucion de todas las funciones anteriores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = (765, 500, 3)\n",
    "\n",
    "# Cargar modelos base\n",
    "model_vgg16 = load_pretrained_model('VGG16', input_shape)\n",
    "model_efficientnet = load_pretrained_model('EfficientNetB0', input_shape)\n",
    "\n",
    "# Configurar modelos completos\n",
    "model_vgg16_v1 = config_model(model_vgg16, version=1)\n",
    "model_vgg16_v2 = config_model(model_vgg16, version=2)\n",
    "model_efficientnet_v1 = config_model(model_efficientnet, version=1)\n",
    "model_efficientnet_v2 = config_model(model_efficientnet, version=2)\n",
    "\n",
    "# ACA\n",
    "# ACA VAN CARGADOS LOS DATOS\n",
    "# ACA\n",
    "train_data = validation_data = None \n",
    "\n",
    "# Entrenamiento\n",
    "history_vgg16_v1 = train_model(model_vgg16_v1, \"VGG16_v1\", train_data, validation_data)\n",
    "history_vgg16_v2 = train_model(model_vgg16_v2, \"VGG16_v2\", train_data, validation_data)\n",
    "history_efficientnet_v1 = train_model(model_efficientnet_v1, \"EfficientNetB0_v1\", train_data, validation_data)\n",
    "history_efficientnet_v2 = train_model(model_efficientnet_v2, \"EfficientNetB0_v2\", train_data, validation_data)\n",
    "\n",
    "# Visualización\n",
    "plot_training_history(history_vgg16_v1, \"VGG16_v1\")\n",
    "plot_training_history(history_vgg16_v2, \"VGG16_v2\")\n",
    "plot_training_history(history_efficientnet_v1, \"EfficientNetB0_v1\")\n",
    "plot_training_history(history_efficientnet_v2, \"EfficientNetB0_v2\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
